# 并发容器

## 简述

- ConcurrentHashMap：线程安全的HashMap；
- CopyOnWriteArrayList：线程安全的List，在**读多写少**的场合性能非常好；
- ConcurrentLinkedQueue：高效的并发队列，使用链表实现。可以看做线程安全的LinkedList，是一个非阻塞队列。
- BlockingQueue：接口，JDK内部使用链表、数组等方式实现这个接口。表示阻塞队列，适合于作为数据共享的通道。
- ConcurrentSkipListMap：跳表。

## ConcurrentHashMap

### 参考

- [HashMap? ConcurrentHashMap? 相信看完这篇没人能难住你！](<https://crossoverjie.top/2018/07/23/java-senior/ConcurrentHashMap/>)

- [ConcurrentHashMap实现分析与使用](<https://zhangzemiao.com/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/ConcurrentHashMap%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90%E4%B8%8E%E4%BD%BF%E7%94%A8/>)

HashMap的线程安全版本。

在ConcurrentHashMap中，读写操作都能保证很高的性能：在进行读操作时（几乎）不需要加锁，在写操作时通过锁分段计数只对所操作的段加锁，不影响对其他段的访问。

### 源码

```java
public class ConcurrentHashMap<K,V> extends AbstractMap<K,V>
implements ConcurrentMap<K,V>, Serializable {
/**
     * The largest possible table capacity.  This value must be
     * exactly 1<<30 to stay within Java array allocation and indexing
     * bounds for power of two table sizes, and is further required
     * because the top two bits of 32bit hash fields are used for
     * control purposes.
     */
    private static final int MAXIMUM_CAPACITY = 1 << 30;

    /**
     * The default initial table capacity.  Must be a power of 2
     * (i.e., at least 1) and at most MAXIMUM_CAPACITY.
     */
    private static final int DEFAULT_CAPACITY = 16;

    /**
     * The largest possible (non-power of two) array size.
     * Needed by toArray and related methods.
     */
    static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;

    /**
     * The default concurrency level for this table. Unused but
     * defined for compatibility with previous versions of this class.
     */
    private static final int DEFAULT_CONCURRENCY_LEVEL = 16;

    /**
     * The load factor for this table. Overrides of this value in
     * constructors affect only the initial table capacity.  The
     * actual floating point value isn't normally used -- it is
     * simpler to use expressions such as {@code n - (n >>> 2)} for
     * the associated resizing threshold.
     */
    private static final float LOAD_FACTOR = 0.75f;

    /**
     * The bin count threshold for using a tree rather than list for a
     * bin.  Bins are converted to trees when adding an element to a
     * bin with at least this many nodes. The value must be greater
     * than 2, and should be at least 8 to mesh with assumptions in
     * tree removal about conversion back to plain bins upon
     * shrinkage.
     */
    static final int TREEIFY_THRESHOLD = 8;

    /**
     * The bin count threshold for untreeifying a (split) bin during a
     * resize operation. Should be less than TREEIFY_THRESHOLD, and at
     * most 6 to mesh with shrinkage detection under removal.
     */
    static final int UNTREEIFY_THRESHOLD = 6;

    /**
     * The smallest table capacity for which bins may be treeified.
     * (Otherwise the table is resized if too many nodes in a bin.)
     * The value should be at least 4 * TREEIFY_THRESHOLD to avoid
     * conflicts between resizing and treeification thresholds.
     */
    static final int MIN_TREEIFY_CAPACITY = 64;

    /**
     * Minimum number of rebinnings per transfer step. Ranges are
     * subdivided to allow multiple resizer threads.  This value
     * serves as a lower bound to avoid resizers encountering
     * excessive memory contention.  The value should be at least
     * DEFAULT_CAPACITY.
     */
    private static final int MIN_TRANSFER_STRIDE = 16;

    /**
     * The number of bits used for generation stamp in sizeCtl.
     * Must be at least 6 for 32bit arrays.
     */
    private static int RESIZE_STAMP_BITS = 16;

    /**
     * The maximum number of threads that can help resize.
     * Must fit in 32 - RESIZE_STAMP_BITS bits.
     */
    private static final int MAX_RESIZERS = (1 << (32 - RESIZE_STAMP_BITS)) - 1;

    /**
     * The bit shift for recording size stamp in sizeCtl.
     */
    private static final int RESIZE_STAMP_SHIFT = 32 - RESIZE_STAMP_BITS;

    /*
     * Encodings for Node hash fields. See above for explanation.
     */
    static final int MOVED     = -1; // hash for forwarding nodes
    static final int TREEBIN   = -2; // hash for roots of trees
    static final int RESERVED  = -3; // hash for transient reservations
    static final int HASH_BITS = 0x7fffffff; // usable bits of normal node hash

    /** Number of CPUS, to place bounds on some sizings */
    static final int NCPU = Runtime.getRuntime().availableProcessors();

    /** For serialization compatibility. */
    private static final ObjectStreamField[] serialPersistentFields = {
        new ObjectStreamField("segments", Segment[].class),
        new ObjectStreamField("segmentMask", Integer.TYPE),
        new ObjectStreamField("segmentShift", Integer.TYPE)
    };
}
```

#### put()/putVal()

```java
    /**
     * 将指定的键映射到指定的值。键和值都不能为null。
     * Maps the specified key to the specified value in this table.
     * Neither the key nor the value can be null.
     *
     * <p>The value can be retrieved by calling the {@code get} method
     * with a key that is equal to the original key.
     *
     * @param key key with which the specified value is to be associated
     * @param value value to be associated with the specified key
     * @return the previous value associated with {@code key}, or
     *         {@code null} if there was no mapping for {@code key}
     * @throws NullPointerException if the specified key or value is null
     */
    public V put(K key, V value) {
        return putVal(key, value, false);
    }

    /** Implementation for put and putIfAbsent */
    final V putVal(K key, V value, boolean onlyIfAbsent) {
        // 键和值都不能为null
        if (key == null || value == null) throw new NullPointerException();
        // 根据key异或获取hash值
        int hash = spread(key.hashCode());
        int binCount = 0;
        // 自旋
        for (Node<K,V>[] tab = table;;) {
            Node<K,V> f; int n, i, fh;
            // 判断是否需要初始化,疑问?初始化的什么,为什么要初始化
            if (tab == null || (n = tab.length) == 0)
                // CAS初始化, 注意这里初始化 并没有 跳出循环
                tab = initTable();
            // f 即为当前 key 定位出的 Node，如果为null表示当前位置的bucket是空的,可以写入数据，利用 CAS 尝试写入，失败则自旋保证成功。
            // tabAt():根据key定位位置
            else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) {
                // casTabAt():插入数据???
                if (casTabAt(tab, i, null,
                             new Node<K,V>(hash, key, value, null)))
                    // 插入成功跳出循环
                    break;                   // no lock when adding to empty bin
            }
            // 走到这里,表明定位位置不为空(之前这里有数据)
            // hash值=MOVED = -1, hash值不正常,
            else if ((fh = f.hash) == MOVED)
                // todo, 没看
                tab = helpTransfer(tab, f);
            // 走到这里,表示hash值正常, 使用sychronized锁住当前节点
            else {
                V oldVal = null;
                synchronized (f) {
                    if (tabAt(tab, i) == f) {
                        // 此时fh是当前节点的hash值
                        if (fh >= 0) {
                            binCount = 1;
                            // fh>0,链式遍历, f是当前节点
                            for (Node<K,V> e = f;; ++binCount) {
                                K ek;
                                // 如果遍历到key匹配,覆盖
                                if (e.hash == hash &&
                                    ((ek = e.key) == key ||
                                     (ek != null && key.equals(ek)))) {
                                    oldVal = e.val;
                                    // put()这个参数传的false,会进入判断
                                    // todo ? 这个参数控制什么?
                                    if (!onlyIfAbsent)
                                        e.val = value;
                                    // 覆盖后会跳出循环
                                    break;
                                }
                                Node<K,V> pred = e;
                                // 判断是否到末尾
                                if ((e = e.next) == null) {
                                    // 如果遍历到末尾都没有key匹配,则将val插入到链表尾部
                                    pred.next = new Node<K,V>(hash, key,
                                                              value, null);
                                    // 插入到链尾也会跳出循环
                                    break;
                                }
                            }
                        }
                        // 此时fh<0,判断f是否被转换为TreeNode(TreeBin实现红黑树)
                        // todo : 有一点疑问?TREEIFY_THRESHOLD在哪里有用到?
                        else if (f instanceof TreeBin) {
                            // 如果链表长度超过或等于8（TREEIFY_THRESHOLD），结点会被转换成TreeNode（由TreeBin实现红黑树）
                            // todo ?没看明白
                            Node<K,V> p;
                            binCount = 2;
                            if ((p = ((TreeBin<K,V>)f).putTreeVal(hash, key,
                                                           value)) != null) {
                                oldVal = p.val;
                                if (!onlyIfAbsent)
                                    p.val = value;
                            }
                        }
                    }
                }
                // 此处转化为红黑树
                if (binCount != 0) {
                    if (binCount >= TREEIFY_THRESHOLD)
                        treeifyBin(tab, i);
                    if (oldVal != null)
                        return oldVal;
                    break;
                }
            }
        }
        // 添加计数
        addCount(1L, binCount);
        return null;
    }

    /**
      * 源自翻译：
      *  散布（XOR）较高位的散列值降低并强制顶部位为0.因为该表使用2次幂掩蔽，仅在当前掩码之上的位变化的散列集将始终发生冲突。 
	 * （在已知的例子中是一组Float键，在小表中保存连续的整数。）因此我们应用一个向下传播高位比特影响的变换。
	 * 在速度，效用和比特扩展质量之间存在权衡。
	 * 因为许多常见的哈希集合已经合理分布（因此不会受益于传播），并且因为我们使用树来处理容器中的大量冲突，所以我们只是以最便宜的方式对一些移位的位进行异或，以减少系统损失，
	 * 以及由于表格边界而包含最高位的影响，否则这些位将永远不会用于索引计算。
     * Spreads (XORs) higher bits of hash to lower and also forces top
     * bit to 0. Because the table uses power-of-two masking, sets of
     * hashes that vary only in bits above the current mask will
     * always collide. (Among known examples are sets of Float keys
     * holding consecutive whole numbers in small tables.)  So we
     * apply a transform that spreads the impact of higher bits
     * downward. There is a tradeoff between speed, utility, and
     * quality of bit-spreading. Because many common sets of hashes
     * are already reasonably distributed (so don't benefit from
     * spreading), and because we use trees to handle large sets of
     * collisions in bins, we just XOR some shifted bits in the
     * cheapest possible way to reduce systematic lossage, as well as
     * to incorporate impact of the highest bits that would otherwise
     * never be used in index calculations because of table bounds.
     */
    static final int spread(int h) {
        return (h ^ (h >>> 16)) & HASH_BITS;
    }

    /**
     * Initializes table, using the size recorded in sizeCtl.
     */
    private final Node<K,V>[] initTable() {
        Node<K,V>[] tab; int sc;
        while ((tab = table) == null || tab.length == 0) {
            if ((sc = sizeCtl) < 0)
                Thread.yield(); // lost initialization race; just spin
            // 此处使用了CAS
            else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) {
                try {
                    if ((tab = table) == null || tab.length == 0) {
                        int n = (sc > 0) ? sc : DEFAULT_CAPACITY;
                        @SuppressWarnings("unchecked")
                        Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n];
                        table = tab = nt;
                        sc = n - (n >>> 2);
                    }
                } finally {
                    sizeCtl = sc;
                }
                break;
            }
        }
        return tab;
    }

```



#### get()

```java
	/**	
	 * 返回指定key映射的值, 没有找到映射的值返回null
     * Returns the value to which the specified key is mapped,
     * or {@code null} if this map contains no mapping for the key.
     *
     * <p>More formally, if this map contains a mapping from a key
     * {@code k} to a value {@code v} such that {@code key.equals(k)},
     * then this method returns {@code v}; otherwise it returns
     * {@code null}.  (There can be at most one such mapping.)
     *
     * @throws NullPointerException if the specified key is null
     */
    public V get(Object key) {
        Node<K,V>[] tab; Node<K,V> e, p; int n, eh; K ek;
        // 根据key获取hash值(二次hash)
        int h = spread(key.hashCode());
        if ((tab = table) != null && (n = tab.length) > 0 &&
        // 根据hash值定位node的位置, e就是找到的node(此时是bucket)
            (e = tabAt(tab, (n - 1) & h)) != null) {
            // 先判断e的hash值是否等于根据key的hash值(value值在bucket上)
            if ((eh = e.hash) == h) {
                // 再判断e的key是否等于传入的key, 如果相等, 则返回e.val
                if ((ek = e.key) == key || (ek != null && key.equals(ek)))
                    return e.val;
            }
            // eh = e.hash, e的hash值<0,代表不在bucket上
            else if (eh < 0)
                // 通过find()链式查找匹配的节点p,找到返回p.val,找不到返回null
                return (p = e.find(h, key)) != null ? p.val : null;
            // 注意, 此时e.hash != key的hash 并且 e.hash >= 0
            while ((e = e.next) != null) {
                if (e.hash == h &&
                    ((ek = e.key) == key || (ek != null && key.equals(ek))))
                    return e.val;
            }
        }
        return null;
    }

        /**
         * Virtualized support for map.get(); overridden in subclasses.
         */
        Node<K,V> find(int h, Object k) {
            Node<K,V> e = this;
            // k是传入的key
            if (k != null) {
                // 链式查找直到最后一个节点
                do {
                    K ek;
                    if (e.hash == h &&
                        ((ek = e.key) == k || (ek != null && k.equals(ek))))
                        return e;
                } while ((e = e.next) != null);
            }
            return null;
        }
```



#### size()

```java
/**
 * 获取map大小
 */
public int size() {
    // 获取计数器的值
    long n = sumCount();
    // 返回0到Integer.MAX_VALUE之间的值
    return ((n < 0L) ? 0 :
            (n > (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE :
            (int)n);
}

final long sumCount() {
    // CounterCell：用于分配计数的填充单元格。改编自LongAdder和Striped64。
    // private transient volatile CounterCell[] counterCells; 
    // 计数用，不为空时，大小为2的幂次
    CounterCell[] as = counterCells; CounterCell a;
    // private transient volatile long baseCount;
    // 基本计数器值，主要在没有竞争时使用；也作为在表初始化时的备用？？。这个值	通过CAS更新。
    long sum = baseCount;
    // as不为空，计数值通过as计算；as为空，返回基本计数值。
    if (as != null) {
        for (int i = 0; i < as.length; ++i) {
            if ((a = as[i]) != null)
                sum += a.value;
        }
    }
    return sum;
}

@sun.misc.Contended static final class CounterCell {
    volatile long value;
    CounterCell(long x) { value = x; }
}
```

#### resizeStamp

```java

```



> 参考： <https://crossoverjie.top/2018/07/23/java-senior/ConcurrentHashMap/>



## CopyOnWriteArrayList

主要针对于**读操作远大于写操作**的场景。因为读操作不会修改原有的数据，所以不用对每次读取进行加锁。在进行读操作时，我们应允许多个线程同时访问List的内部数据。

CopyOnWriteArrayList的读取操作时**完全**不用加锁的，同时写入也不会阻塞读取操作，只有在**写入和写入之间**需要进行同步等待。

### 读取操作

全程未加锁，内部array数组不会更改，只会被另一个array替换。

···

### 写入操作

add()在添加集合时加锁，避免多线程操作时拷贝多个副本出来。

···

## ConcurrentLinkedQueue

### 阻塞队列与非阻塞队列

Java中线程安全的Queue分为**阻塞队列**和**非阻塞队列**。

阻塞队列的代表：**BlockingQueue**。

非阻塞队列的代表：**ConcurrentLinkedQueue**。

### ConcurrentLinkedQueue

使用链表作为底层数据结构。底层主要使用**CAS非阻塞算法**来实现线程安全。

ConcurrentLinkedQueue适用于对性能要求相对较高，同时存在多线程队列读写同时进行的场景。

## BlockingQueue

阻塞队列BlockingQueue广泛应用于**生产者-消费者**问题中，原因是BlockingQueue提供了可阻塞的插入和移除方法。当队列容器已满，生产者线程会被阻塞，知道队列处于未满状态；当队列为空时，消费者线程会被阻塞，知道队列为非空。

BlockingQueue是一个继承自Queue的接口。实现类可以作为Queue的实现来使用。

### ArrayBlockingQueue

BlockingQueue的有界队列实现类，底层采用**数组**实现。

### LinkedBlockingQueue

基于**单向链表**实现的阻塞队列，即可当做**无界队列**也可以当做**有界队列**来使用。

### PriorityBlockingQueue

支持优先级的无界阻塞队列。

## ConcurrentSkipListMap

### 跳表

跳表是一种空间换时间的算法。