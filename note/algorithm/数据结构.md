# 数据结构

## 复杂度

衡量算法的执行效率:
- 时间复杂度(运行快)
- 空间复复杂度(空间省)  

简单粗暴的统计：**事后统计法**（直接运行一遍代码获取执行时间和占用内存大小），局限性：
- 测试结果非常依赖测试环境
- 测试结果受数据规模的影响很大  

综上，我们需要不用具体测试数据测试就能粗略地估计算法执行效率的方法：**时间复杂度**、**空间复杂度**分析方法。

### 大O复杂度表示法
```math
    T(n) = O(f(n))
```
T(n)表示代码执行时间；  n表示数据规模大小；  f(n)表示每次代码执行次数总和（一个公式，所以用f(n)表示）；  O表示代码执行时间T(n)与f(n)表达式成正比。  e.g:

```math
T(n) = O(2n^2 + 2n + 3)
```
用上面式子表示复杂度的方法就叫做**大O时间复杂度表示法**。  

大O时间复杂度实际并不表示代码真正的执行时间，而是表示**代码执行时间随数据规模增长的变化趋势**，所以这里大O表示的复杂度也叫做**渐进时间复杂度（asymptotic time complexity）**，我们一般简称为时间复杂度。  

在上面式子中，当n趋近于无穷大时，公式中的系数、低阶、常量并不左右变化趋势。在大O表示法中，我们取最大量级表示上述式子的时间复杂度，记为：

```math
T(n) = O(n^2)
```
### 时间复杂度
#### 时间复杂度分析
- 只关注循环执行次数最多的一段代码：大O表示法只表示一种变化趋势。我们在分析算法时，也只需关注循环次数最多的一段代码。
e.g.
```java
private int calculator(int n) {
    int sum = 0;
    for (int i = 0; i < n; i++) {
        sum += i;
    }
    return sum;
}
```
上面代码中，*int sum = 0*执行了常量次，for中的循环执行了n次，所以该方法的时间复杂度为O(n);
-  加法法则：总复杂度等于量级最大的那段代码的复杂度（在能比较量级的情况下）。e.g.
```java
private int calculator(int n) {
    // first
    int sum1 = 0;
    for (int i = 0; i < 10000; i++) {
        sum1 += i;
    }

    // second
    int sum2 = 0;
    for (int i = 0; i < n; i++) {
        sum2 += i;
    }

    // third
    int sum3 = 0;
    for (int i = 1; i <= n; i++) {
        for (int j = 1; j <= n; j++) {
            sum3 += i * j;
        }
    }

    return sum1 + sum2 + sum3;
}
```
上面方法中可以分成三部分来计算：  
计算sum1时，虽然for循环代码执行了10000次，但也是一个常量执行时间，与n无关  
计算sum2时，循环执行了n次，所以代码时间复杂度为O(n)。  
计算sum3时，内层循环执行了`$n^2$`次，时间复杂度为`$O(n^2)$`。  
综上，我们取三个计算的最大量级，上面方法的时间复杂度为`$O(n^2)$`。  
将加法法则抽象为公式：
设`$T_1(n) = O(f(n)), T_2(n) = O(g(n))$`, 所以
```math
T(n) = T_1(n) + T_2(n) = max(O(f(n), O(g(n))) = O(max(f(n), g(n)))
```
- 乘法法则：**嵌套**代码的复杂度等于嵌套内外代码复杂度的乘积。  
e.g.
```java
private int outerCalculator(int n) {
    int sum = 0;
    for (int i = 0; i < n; i++) {
        sum = i + innerCalculator(i);
    }
    return sum;
}

private int innerCalculator(int n) {
    int sum = 0;
    for (int i = 0; i < n; i++) {
        for (int j = 0; j < n; j++) {
            sum += i * j;
        }
    }
    return sum;
}
```
在上面示例中，假设innerCalculator()只是一个简单的操作，那么outerCalculator()的时间复杂度为`$T_1(n) = O(n)$`;但是innerCalculator()并不是一个简单的操作，它的时间复杂度为`$T_2(n) = O(n^2)$`，结合乘法法则我们可以知道outerCalculator()实际的时间复杂度为：`$T(n) = T_1(n) * T_2(n) = O(n) * O(n^2) = O(n^3)$`  
我们用公式来表示乘法法则：设`$T_1(n) = O(f(n)), T_2(n) = O(g(n))$`,所以
```math
T(n) = T_1(n) * T_2(n) = O(f(n)) * O(g(n) = O(f(n) * g(n))
```
#### 几种常见的复杂度量级

![常见的复杂度量级](./imgs/常见的复杂度量级.jpg)

##### 多项式量级

- 常量阶 `$O(1)$`
- 对数阶 `$O(logn)$`
- 线性阶 `$O(n)$`
- 线性对数阶 `$O(nlogn)$`
- 平方阶 `$O(n^2)$`
- 立方阶 `$O(n^3)$` ··· k次方阶 `$O(n^k)$`
##### 非多项式量级

- 指数阶 `$ O(2^n)$`
- 阶乘阶 `$O(n!)$`

1. O(1)

首先需要明确的是，O(1)只是常量阶时间复杂度的表示方法，其中的1不表示只执行了一行代码。  

只要代码的执行时间不随n的增长而增长，这样的代码我们都可以说它的时间复杂度为O(1)， 与代码执行的行数无关。

2. O(n)/O(nlogn)

先看一个示例：
```java
 private int calculator(int n) {
    int i = 1;
    while (i <= n) {
        i *= 3; // i = i * 2, 时间复杂度log_2n, log_3n和log_2n均为O(logn)
    }
    return i;
}
```
从上示代码可以看出，i从1开始，每次循环乘3，直到结果大于n。变量i的取值其实是一个等比数列。假设代码的执行次数为x。我们可以得到：
```math
3^x = n;  

x = log_3n
```
所以，上面示例中的时间复杂度为`$O(log_3n)$`。   

又由于**对数的换底公式**： 

对于``$ a,c∈(0,1)∪(1,+ ∞)$ ``且``$b∈(0,+∞)$``,有:

$$
log_ab = \frac{log_cb}{log_ca}
$$
以及对数的性质:
$$
log_ab * log_ba = 1
$$
可推得:
$$
log_3n = \frac{log_2n}{log_23} = log_2n * log_32
$$
由于``$log_32$``是一个常量，我们在用大O表示法表示复杂度时是忽略常量系数的，所以上面示例中最终的时间复杂度为``$T(n) = O(log_2n) = O(logn) $``（以2为底的对数我们通常记为``$logn$``）

由上面的乘法法则我们可以知道时间复杂度为O(nlogn)的代码其实就是时间复杂度为O(logn)的代码循环执行n次。

```java
// 整个方法时间复杂度O(n) * O(logn) = O(nlogn) 
private int calculator(int n) {
    int i = 1;
    // 外层时间复杂度O(n)
    for (int times = 0; times < n; times++) {
        // 内层时间复杂度O(logn)
        while (i <= n) {
            i *= 3; // i = i * 2, 时间复杂度log_2n, log_3n和log_2n均为O(logn)
        }
    }
    return i;
}
```

3. O(m+n)/O(m*n)

在几个复杂度相加的情况下，我们由上面的加法法则可以知道，在能比较量级的情况下，我们取量级大的复杂度为最后的复杂度。但是如果不能比较，我们该怎么算复杂度呢？
```java
private int calculator(int m, int n) {
    int sum1 = 0;
    for (int i = 0; i < m; i++) {
        sum1 += i;
    }

    int sum2 = 0;
    for (int i = 0; i < n; i++) {
        sum2 += i;
    }

    return sum1 + sum2;
}
```
在上面例子中，m和n表示两个数据规模，我们无法比较m和n谁的量级更大，所以这时就不能用加法法则来取其中一个的时间复杂度，所以上面的时间复杂度为O(m+n)。 

而对于2个数据规模而言，乘法法则仍然有效，``$T_1(m)*T_2(n) = O(f(m) * f(n))$``

```java
// 整个方法的时间复杂度 O(m) * O(n)
private int calculator(int m, int n) {
	int sum = 0;
    // 外层时间复杂度O(m)
    for (int outter = 0; outter < m; outter++) {
        // 内层时间复杂度O(n)
        for(int inner = 0; inner < n; inner++) {
            sum += inner;
        }
    }
    return sum;
}
```

##### 复杂度函数图

![常见的复杂度图示](.\imgs\复杂度函数图.jpg)



当同一段代码在不同的情况下会出现**量级上的差异**，为了更全面、更准确的描述代码的时间复杂度，引入了最好时间复杂度、最坏时间复杂度、平均时间复杂度、均摊时间复杂度4个概念。

#### 最好/最坏时间复杂度

最好时间复杂度：在最理想的情况下执行代码的时间复杂度。

最坏时间复杂度：在最糟糕的情况下执行代码的时间复杂度。

```java
// n表示数组array的长度
int find(int[] array, int n, int x) {
  int i = 0;
  int pos = -1;
  // n为array.length 此处显示声明只是为了便于说明
  for (; i < n; ++i) {
    if (array[i] == x) {
       pos = i;
       break;
    }
  }
  return pos;
}
```

上面例子中，最理想的情况就是第一次就找到x的position，最好时间复杂度为`O(1)`；最糟糕的情况是遍历完都没有找到，时间复杂度为`O(n)`。

#### 平均时间复杂度

平均时间复杂度就是要把各种情况发生的概率考虑进去。

加上概率之后算出来的值就是概率论中的**加权平均值**，也叫做**期望值**。所以平均时间复杂度也叫作加权时间复杂度或者期望时间复杂度。

我们通过**取加权平均值**来算继续上述的例子，**假设**x在array中概率为`1/2`，不在array中概率也是`1/2`。那么在for循环中，在i从0到n-1的过程中找到x的概率依次为`1/2 x 1/n`,`1/2 x 2/n`...`1/2 x n/n`，此外在加上循环完都找不到的概率`1/2 x n/n`(找不到是建立在循环完毕的基础上的)，所以，find方法的时间复杂度计算为：

![平均时间复杂度计算](./imgs/平均时间复杂度计算.jpg)

所以，经过加权后的时间复杂度仍然为`O(n)`。

通常最好、最坏、平均复杂度只有**同一块代码在不同的情况下，时间复杂度有量级的差距**的情况下才会用到。

#### 均摊时间复杂度

均摊复杂度程出现的场景比上述三种复杂度出现的场景更特殊、更有限。

来看一段`ArrayList`的源码：

```java
    public void add(int index, E element) {
        // 范围检测，可忽略
        rangeCheckForAdd(index);
        // 修改次数自增，可忽略
        modCount++;
        // s = size
        final int s;
        // 实际保存数据的数组
        Object[] elementData;
        
        // 只有在size=elementData.length的情况下才会执行grow(), 假设这里复杂度O(n)
        if ((s = size) == (elementData = this.elementData).length) {
           elementData = grow();
        }
        System.arraycopy(elementData, index,
                         elementData, index + 1,
                         s - index);
        // size<elementData.length时执行， 复杂度O(1)
        elementData[index] = element;
        size = s + 1;
    }
```

上面代码中，在绝大多数情况下时间复杂度都为`O(1)`，在极端情况下复杂度才为`O(n)`。此外，`O(n)`复杂度的算法总是出现在一串连续（n-1次）的`O(1)`复杂度算法之后。 我们可以把第n次`O(n)`的操作均摊到之前的n-1次`O(n)`的操作上，所以上述方法的时间复杂度就为`O(1)`。

类似上述分析的方法我们称为**摊还分析法**，通过摊还分析法计算的时间复杂度就叫**均摊时间复杂度**。

均摊复杂度实际上是一种特殊的平均复杂度。

>  对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个时候，我们就可以将这一组操作放在一块儿分析，看是否能将较高时间复杂度那次操作的耗时，平摊到其他那些时间复杂度比较低的操作上。而且，在能够应用均摊时间复杂度分析的场合，一般均摊时间复杂度就等于最好情况时间复杂度。

### 空间复杂度

空间复杂度的全称是**渐进空间复杂度(asymptotic space complexity)**，表示算法的**存储空间与数据规模**之间的增长关系。

```java

void print(int n) {
    // 变量i与n无关， O(1)空间复杂度
    int i = 0;
	// 数组a与n相关，申请了一个大小为 n 的 int 类型数组, O(n)空间复杂度
    int[] a = new int[n];
    
    for (i; i <n; ++i) {
      a[i] = i * i;
    }
}
```

常见的空间复杂度：`O(1)`,`O(n)`以及`O(n^2)`。

## 数组

>  数组(Array)是一种**线性表**数据结构。它用一组**连续的内存空间**，来存储一组具有**相同类型的数据**。

### 线性表

> 线性表(Linear List)表示数据排成一条线一样的结构。每个线性表伤的数据最多只有前和后两个方向。数组，链表，队列，栈等都是线性表结构。
#### 线性表
![线性表](.\imgs\线性表.jpg)

#### 非线性表
> 与线性表对应的结构是**非线性表**，比如二叉树，堆，图等结构。在非线性表中，数据之间的排列并不是简单的前后关系。

![非线性表](.\imgs\非线性表.jpg)

#### 连续的内存空间和相同的类型数据
正因为有了这两个特性，数组才有了**随机访问**的特性。但是也正式因为这两个特性，也使得数组的删除/插入操作变得非常低效。如果想要**保持数组的连续性**，在数组中插入或删除一个数据会做大量的数据迁移工作。
### 寻址公式
![数组连续的地址](.\imgs\寻址公式.jpg)
拿一个长度为10的int类型数组举例：

```java
int[] a = new int[10];
```
图中，假如计算机给数组a[10]分配了一块**连续的内存空间**1000~1039，其中，内存块的首地址为base_address = 1000。

我们知道计算机会给每个内存单元分配一个地址，计算机通过地址来访为内存中的数据。 当计算机需要随机访问数组中的某个元素时，首先会通过下面的**寻址公式**，计算出该元素存储的内存地址：

```mathematica
a[i]_address = base_address + i * data_type_size;
```
上例中存储的是int类型的数据，所以data_type_size是4个字节。  

同理，m x n的二维数组的寻址公式为：

```mathematica
// i < m, n < j
a[i][j] = a[0][0] + (i * n + j) * data_type_size;
```

#### 数组和链表的区别
纠正一种不准确的说法：
>  数组适合查找，查找的时间复杂度为O(1)；链表适合插入和删除，插入和删除的时间复杂度为O(1)。  

事实上，数组虽然适合查找操作，但是查找的时间复杂度并不为O(1)。即便是排好序的数组，用二分查找法查找，时间复杂度也是`$O(logn)$`。  

对数组准确的描述应该是：数组支持随机访问，**根据下标随机访问**的时间复杂度是O(1)。

#### 为什么数组下标要从0开始编号
从数组的内存模型来看，**下标**最确切的定义应该是**偏移(offset)**。上面讲到，如果用a来表示素组的首地址，a[0]就表示偏移k个data_type_size的地址，偏移为0的地址也就是**首地址**，而a[k]就表示偏移k个data_type_size的位置，所以a[k]的内存地址可以用如下公式来计算:
```java
a[k]_address = base_address + K * data_type_size
```
假如数组从1开始计数，那么a[k]的内存地址就会变成:
```java
a[k]_address = base_address + (K - 1) * data_type_size
```
对比而言，如果数组下标从1开始编号，每次随机访问数组元素都多了一次减法运算，对CPU来说就多了一次减法指令。
### 数组的插入和删除
#### 插入
假设存在一个数组长度为n，如果我们要将一个数据插入到数组中的第k个位置(k<n)。在保证之前顺序的情况下，为了把第k个位置挪出来给新插入的数据，我们需要将第k~n部分的数据都顺序的往后挪一位。  

最好情况，如果是在数组的末尾插入元素，此时不需要移动数据，此时的时间复杂度为O(1)。 

最坏情况，如果实在数组的开头插入元素，此时所有的数据都需要往后挪一位，最坏的时间复杂度为O(n)。

由于往每个位置插入数据的概率是一样的，所以插入数据的平均时间复杂度为(1+2+···n)/n = O(n)。  

***
如果数组中存储的数据不需要任何规律，只是作为数据存储的一个集合。在这种情况下，我们可以避免大规模的数据转移，直接将第k位的数据挪到数组元素的最后，然后将插入的元素直接放入第k位。
![避免大规模数据迁移的数组插入](.\imgs\数组的插入.jpg)
在上述场景中，在第k位插入元素的时间复杂度就会降为O(1)。这种处理思想在**快速排序**中有很好的应用。

#### 删除
在保证内存连续性的情况下，如果我们要删除第k个位置的数据，也需要搬移大量的数据。  

与插入类似，删除末尾元素，时间复杂度为O(1)；删除开头的元素，最坏时间复杂度为O(n);平均时间复杂度为O(n)。  

在某些场景下，我们并不一定非得追求数组中数据的连续性。如果我们将多次删除操作集中在一起执行，删除的效率会高很多。
![数组删除操作](.\imgs\数组批量删除.jpg)

在图中，假设数组a[10]存储了8个元素：a到h。我们依次删除a,b,c3个元素。为了避免d,e,f,g,h这几个数据被搬移3此，我们可以先记录下已经删除的数据a,b,c。每次的删除操作并不是真正的搬移数据，而只是记录数据已被删除。当数组真正**没有更多空间存储数据**时，我们再触发一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。  

上面的思想正是**JVM标记清除垃圾回收算法**的核心思想。

### 数组访问越界的问题
- java：抛出java.lang.ArrayIndexOutOfBoundException;
- C: 下面代码会无限循环（c语言中，只要不是访问受限的内存，其他内存都可以自由访问）
```c
int main(int argc, char* argv[]){
    int i = 0;
    int arr[3] = {0};
    for(; i<=3; i++){
        arr[i] = 0;
        printf("hello world\n");
    }
    return 0;
}
```
根据寻址公式，越界的元素a[3]会被定位到某块不属于数组的内存地址上，而这个地址正好是存储变量i的内存地址，那么a[3]=0就等同于i=0，所以会导致循环内代码停不下来。  

数组越界在c语言中是一种未决行为，并没有规定数组访问越界时编译器该如何处理。因为访问数组的本质就是访问一段连续内存，只要数组通过偏移计算得到的内存地址时可用的，那么程序就有可能不报任何错误。

### 数组与容器（ArrayList）的选择 

数组对应的容器类ArrayList无法储存基本数据类型，需要封装成包装类，而自动装箱、拆箱（AutoBoxing、Unboxing）会有一定的性能开销，如果追求极致的性能，可以优选数组。

> 为什么容器不能存储基本数据类型？ 
>
> > 网上看了很多答案，感觉都不怎么有说服力，总结一下大致可能的原因：
> >
> > - 容器存储的是Object对象的子类，而基本数据类型不是Object对象的子类
> > - [Java本身的设计决策，容器需要对象，而基本类型不是对象](https://stackoverflow.com/questions/2504959/why-can-java-collections-not-directly-store-primitives-types)：Java在编译的时候提供了对类的支持，而在JVM中不支持"非对象"。

如果数据大小是确定的，并且用不到ArrayList类的绝大部分方法，可以选择数组。

多维数组用数组（`int [] []`）体现比容器（`ArrayList<ArrayList>`）更为直观。

### 代码实现 // todo

#### 实现一个支持动态扩容的数组

```java

```

#### 实现一个大小固定的有序数组(支持动态增删改)

```java

```

#### 实现两个有序数组合并成一个有序数组

```java

```



## 链表

>  链表也是一种**线性表**，但是在存储方面不会按线性的存储顺序存储数据，而是在每一个结点里存储指向下一个节点的指针（Pointer）。

### 数组与链表的比较

从底层的存储结构来区分:

> 数组需要一块**连续的内存空间**来存储数据，对内存要求比较高。注意这里要求内存空间必须是**连续的**，假如我们要申请一个100MB大小的数组，不管内存有多大剩余空间，只要没有连续的100MB大小内存，都是会申请失败的。
>
> 相比而言，链表不需要连续的内存空间。链表通过**指针**将一组零散的内存块串起来使用。如果我们申请的是100MB大小的链表，只要内存有100MB大小（不管它是否连续）就可以。  

下面是数组和链表存储结构对比图：

![数组链表的存储结构对比](.\imgs\数组和链表的内存分布.jpg)

数组支持快速查询，数组适用于有索引语义的情形。

### 链表结构

#### 单链表
![单链表结构示意图](.\imgs\单链表.jpg)
##### 结点和后继指针
- 结点： 上面说链表通过指针将一组零散的内存块串在一起。这里的内存块就称之为链表的**结点**
- 后继指针： 每个链表的结点除了存储数据之外，还需要记录链上下一个结点的地址。这个记录下个结点地址的指针就叫做**后继指针next**。

##### 头结点和尾结点
- 头结点： 链表的第一个结点，用来记录链表的基地址。有了头结点就可以遍历得到整条链表。
- 尾结点： 链表的最后一个结点。尾结点的指针不是指向下一个结点，而是指向一个**空地址NULL**，表示这是链表上最后一个结点。

##### 链表的查找、插入和删除操作
- 链表的插入和删除

![链表的插入和删除](.\imgs\链表的插入和删除.jpg)
在进行数组的插入和删除时，因为要保证内存数据的连续性，需要做大量的搬移操作，数组插入和删除的时间复杂度为O(n)。  
而在对链表插入和删除时，只需考虑相邻结点指针的变化，时间复杂度为O(1)。

-  链表的随机访问

根据寻址公式，数组根据下标随机访问的时间复杂度为O(1)。  
而对链表而言，因为链表的数据并不是连续存储的，随机访问时，我们只能根据指针一个一个结点地遍历。随机访问链表的时间复杂度为O(n)。

#### 循环链表
> 循环链表是一种特殊的单链表。循环链表和单链表的区别就在与尾结点。单链表的尾结点指向一个**空地址**，而循环链表的尾结点指向链表的**头结点**。 

![循环链表结构图](.\imgs\循环链表.jpg)
与单链表相比，循环链表的优点是：从链尾到链头比较方便。当处理的数据具有环型的结构特点时（比如[约瑟夫问题](https://zh.wikipedia.org/wiki/%E7%BA%A6%E7%91%9F%E5%A4%AB%E6%96%AF%E9%97%AE%E9%A2%98)），就适合使用循环链表的结构。

此外，如果我们想要保存数量固定的最新数据时通常可以选择使用循环

####  双向链表
> 双向链表，每个结点不只有一个**后继指针next**指向后面的结点，还有一个**前驱指针prev**指向前面的结点。

![双向链表结构图](imgs\双向链表.jpg)
除了内存块之外，双向链表需要额外的两个空间来存储后继结点和前驱结点的地址。存储同样多的数据，双向链表比单链表更占用内存空间。  

此外，双向链表在进行增加或删除操作时需要改变更多指针的指向。

#### 链表的操作

##### 删除和插入操作
- 删除结点中“值等于某个特定值”的结点：
> 对于此种情况，不管是单链表还是双向链表，为了查找值等于给定值的结点（可能有多个），都需要从头结点考试一个个地遍历对比，直到找到符合条件的结点删除。  
> 虽然链表删除操作的时间复杂度为O(1)，但删除之前需要遍历查找（时间复杂度为O(n)）, 所以此种情况的链表删除操作时间复杂度为O(n)。
- 删除给定指针指向的结点：
> 这里我们已经知道了要删除的结点，但是删除某个结点q需要知道它的前驱结点。在单链表中，不支持直接获取前驱结点，为了找到前驱结点，还是需要从头开始遍历链表，知道p-next->q，才说明p是q的前驱结点。单链表删除时间复杂度为O(n)。  
> 对双向链表而言，双向链表的结点中已经保存了前驱结点的指针，所以并不需要像单链表一样去遍历一遍。针对这种情况的双向链表删除操作，时间复杂度为O(1)。

插入同理，双线链表**在指定结点前插入**一个结点，时间复杂度为O(1)，单链表为O(n)。

##### 查询
> 对于一个**有序链表**，双线链表的按值查询的效率也要比单链表高。因为有序，我们查询时可以记录上次查询的位置p，先比较查询值与p的大小关系，根据大小关系决定是往前查找还是往后查找，比起单链表，平均只需查找一半的数据。时间复杂度为O(n/2)=O(n)。 

LinkedHashMap的实现原理就又到了双向链表这种数据结构。

### 空间换时间的设计思想
> 在实际开发中，虽然双向链表比较费内存，但是使用还是比单链表更加广泛。当内存空间充足的时候，如果我们追求代码的执行速度，可以选择空间复杂度相对较高、但时间复杂度低的算法或者数据结构。反之是**时间换空间**的设计思想。

缓存的例子也是利用了空间换时间的设计思想。数据若是存在硬盘内，节省内存但是每次访问数据都要访问硬盘，比较耗时。通过缓存，查询一次之后将数据存在内存内，虽然耗费了内存空间，但是访问硬盘的时间大大减小了。

## 栈

当某个数据集合**只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性**，应该优先选择**栈**这个数据结构操作数据。

栈也是一种线性表数据结构。

![栈结构](./imgs/栈-栈结构.jpg)

### 栈的代码实现

#### 顺序栈

顺序栈是用数组实现的栈。

```java
/**
 * @className: ArrayStack
 * @author: yang
 * @date: 2019/11/5
 */
@Data
public class ArrayStack {

    /**
     * 存放元素的素组
     */
    private Object[] array;

    /**
     * 栈实际大小
     */
    private int count;

    /**
     * 数组大小
     */
    private int size;

    /**
     * 构造方法
     * @param size
     */
    public ArrayStack(int size) {
        this.array = new Object[size];
        this.count = 0;
        this.size = size;
    }

    /**
     * 栈顶入栈
     * 不扩容时,时间复杂度O(1);需要扩容时,时间复杂度O(n);均摊时间复杂度O(1)
     * @param obj
     * @return
     */
    public Object push(Object obj) {
        if (size > count + 1) {
            array[count] = obj;
        } else {
            // 扩容2倍
            size = size << 1;
            array = Arrays.copyOf(array, size);
            array[count] = obj;
        }
        count++;
        return obj;
    }

    /**
     * 栈顶出栈
     * 时间复杂度O(1)
     * @return
     */
    public Object pop() {
        Object obj = null;
        if (count > 0) {
            obj = array[count - 1];
            array[count - 1] = null;
            count--;
        }
        return obj;
    }

    /**
     * 返回栈顶元素
     * @return
     */
    public Object peek() {
        if (count == 0) {
            return null;
        }
        return array[count -1];
    }
}

```

用摊还分析法分析支持动态扩容的入栈时间复杂度:

![入栈时间复杂度](./imgs/栈-入栈时间复杂度.jpg)

#### 链式栈

链式栈是用链表实现的栈

```java
/**
 * @className: LinkedStack
 * @author: yang
 * @date: 2019/11/5
 */
@Data
public class LinkedStack<E> {

    /**
     * 栈追上层节点
     */
    private Node<E> header;

    /**
     * Node节点个数
     */
    private int count;


    /**
     * Node内部类
     * @param <E>
     */
    @Data
    private static class Node<E> {
        /**
         * 节点值
         */
        private E item;
        /**
         * 下个节点
         */
        private Node<E> next = null;

        Node(E item) {
            this.item = item;
        }
    }

    /**
     * 压入栈
     * @param e
     * @return
     */
    public E push(E e) {
        if (header == null) {
            header = new Node<>(e);
        } else {
            Node<E> old = header;
            Node<E> newNode = new Node<>(e);
            newNode.setNext(old);
            header = newNode;
        }
        count++;
        return e;
    }

    /**
     * 获取栈顶节点
     * @return
     */
    public E peek() {
        if (header == null) {
            return null;
        }
        return header.item;
    }

    /**
     * 弹出栈
     * @return
     */
    public E pop() {
        if (header == null) {
            return null;
        } else {
            // 不能使用Node curNode = header; 
            // curNode和header是同一个引用,header引用的对象修改了,curNode也会被修改
            E curItem = header.item;
            Node<E> nextHeader = header.next;
            header.setItem(null);
            header.setNext(null);
            header = nextHeader;
            count--;
            return curItem;
        }

    }

}
```

### 栈的应用

#### Java虚拟机栈

Java虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行的同时都会创建一个栈帧（Stack Frame）用于存储局部变量表、操作数栈、动态链接、方法出口等信息。

**每一个方法从调用到执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程**。

##### 局部变量表

```java

int main() {
   int a = 1; 
   int ret = 0;
   int res = 0;
   ret = add(3, 5);
   res = a + ret;
   printf("%d", res);
   reuturn 0;
}

int add(int x, int y) {
   int sum = 0;
   sum = x + y;
   return sum;
}
```

下面是main方法执行到add()函数时函数调用栈情况:

![函数调用栈](./imgs/栈-函数调用栈图例.jpg)

##### 操作数栈

编译器通过操作数栈和局部变量表来实现表达式求值。

> 从左向右遍历表达式，当遇到数字，我们就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶元素进行比较。
>
> 如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。

通过两个栈进行表达式求值的图例:

![表达式求值](./imgs/栈-表达式求值.jpg)

#### 括号匹配

> 我们用栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号时，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，比如“(”跟“)”匹配，“[”跟“]”匹配，“{”跟“}”匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。

## 队列

## 哈希表

## 堆

## 二叉查找树

### 二叉树

- 动态数据结构
- 二叉树具有唯一根节点
- 每个节点最多有两个子节点
- 每个节点最多有一个父节点
- 具有天然的递归结构
  - 每个节点的左子树也是二叉树
  - 每个节点的又子树也是二叉树
- 不满的二叉树
  - 一个节点也是二叉树
  - 空也是二叉树

### 二分搜索树

- 二分搜索树也是二叉树
- 二分搜索树**每个节点**的值：
  - 大于其左子树所有节点的值
  - 小于其右子树所有节点的值
- 存储的元素必须具有可比较性（为了满足上述条件）

#### 二分搜索树的遍历

##### 前序遍历

最自然、最常用的遍历方式

- 访问该节点
- `traverse(node.left)`
- `traverse(node.right)`

##### 中序遍历

- `traverse(node.left)`
- 访问该节点
- `traverse(node.right)`

##### 后续遍历

- `traverse(node.left)`
- `traverse(node.right)`
- 访问该节点

##### 层序遍历（广度优先遍历）

广度优先遍历的意义：

- 更快的找到问题的解
- 常用算法设计中：无权图最短路径问题
- 图中的深度优先遍历和广度优先遍历

#### 二分搜索树的应用

- 二分搜索树的顺序性
- 二分搜索树的floor和ceil
- 维护size的二分搜索树
- 维护depth的二分搜索树
- 支持重复元素的二分搜索树
- 基于二分搜索树的集合实现

#### 二分搜索树的复杂度分析

| 操作       | LinkedListSet | BSTSet                        |
| ---------- | ------------- | ----------------------------- |
| add()      | O(n)          | O(h)，h=BST的高度， = O(logn) |
| contains() | O(n)          | O(h)                          |
| remove()   | O(n)          | O(h)                          |



## 排序

![排序算法比较](./imgs/排序-排序比较.jpg)

### 如何分析排序算法

#### 执行效率

1.  最好、最坏、平均情况时间复杂度
2.  时间复杂度的系数、常数、低阶
3.  比较次数和交换（移动）次数

#### 内存消耗

算法的内存消耗可以通过空间复杂度来衡量。

**原地排序算法**（Sorted In Place），就是特指空间复杂度为`O(1)`的算法。

冒泡、插入、选择排序都是原地排序算法。

#### 稳定性

算法的稳定性是指：如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变。

### 冒泡排序（Bubble Sort）

冒泡排序只会操作相邻的两个数据。

每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。一次冒泡会让至少一个元素移动到它应该在的位置，重复 n 次，就完成了 n 个数据的排序工作。

初始版本：

![冒泡初始版本](./imgs/排序-冒泡排序初始版.jpg)

当某次冒泡没有数据交换时，说明已经达到完全有序，不用再继续执行冒泡操作。

进阶版本：

![冒泡进阶版本](./imgs/排序-冒泡进阶版本.jpg)



## 参考

- [数据结构与算法之美](https://time.geekbang.org/column/intro/126)
- [程序员的数学基础课](https://time.geekbang.org/column/intro/143)